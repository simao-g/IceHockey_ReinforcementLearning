{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03db1fd4",
   "metadata": {},
   "source": [
    "# **Customising Gymnasium Environments and Implementing Reinforcement Learning Agents with Stable-Baselines3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770080f",
   "metadata": {},
   "source": [
    "*Trabalho por*: Inês Castro (202304060), Soraia Costa (202305078), Simão Gomes (202304752)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4280580",
   "metadata": {},
   "source": [
    "2.3 : descobrir como fazer citações em condições :O <br>\n",
    "Tá aqui só para não ficar perdido lá no meio <br>\n",
    "<br>\n",
    "As métricas extraídas, comparadas e analisadas serão as seguintes:\n",
    "- TEMPO MÉDIO GOLO MARCADO - avalia a capacidade ofensiva dos agentes\n",
    "- TEMPO MÉDIO GOLO SOFRIDO - avalia a capacidade defensiva dos agentes\n",
    "- DIFERENÇA DE GOLOS MÉDIA (<0, adversário ganha; >0, agentes ganham) - comparação com os agentes adversários\n",
    "- TEMPO POSSE DO DISCO - se tiver pouco, não está a tocar no disco; se tiver muito não está a rematar, será avaliado em conjunto com o tempo adversário\n",
    "- TEMPO ADVERSÁRIO POSSE DO DISCO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778e1f7",
   "metadata": {},
   "source": [
    "***\n",
    "# **índice**\n",
    "\n",
    "### 1. Introduction\n",
    "   - 1.1. Objective\n",
    "   - 1.2. Environment Selection: IceHockey-v5\n",
    "   - 1.3. Motivation and Expected Outcomes\n",
    "\n",
    "### 2. Environment Analysis\n",
    "   - 2.1. Original Environment Overview\n",
    "        - Observation Space\n",
    "        - Action Space\n",
    "        - Reward Structure\n",
    "        - Game Mechanics\n",
    "   - 2.2. Baseline Performance Analysis\n",
    "        - Random Agent Behavior\n",
    "   - 2.3. Identified Modification Opportunities\n",
    "\n",
    "### 3. Environment Customization (Week 1)\n",
    "   - 3.1. Reward Engineering Design\n",
    "        - Proposed Modifications\n",
    "        - Theoretical Justification\n",
    "   - 3.2. Implementation\n",
    "        - Custom Wrapper Code\n",
    "        - Testing and Validation\n",
    "   - 3.3. Modified Environment Behavior\n",
    "        - Expected vs Actual Changes\n",
    "        - Edge Cases Handling\n",
    "\n",
    "### 4. Baseline Agent (Week 2)\n",
    "   - 4.1. Algorithm Selection\n",
    "        - Why DQN/PPO?\n",
    "        - Alternative Considerations\n",
    "   - 4.2. Baseline Configuration\n",
    "        - Hyperparameters (Default)\n",
    "        - Training Setup\n",
    "   - 4.3. Baseline Training\n",
    "        - Training Process\n",
    "        - Convergence Analysis\n",
    "   - 4.4. Baseline Results\n",
    "        - Performance Metrics\n",
    "        - Behavior Analysis\n",
    "\n",
    "### 5. Custom Environment Agent (Week 2)\n",
    "   - 5.1. Training with Custom Rewards\n",
    "        - Same Algorithm, New Environment\n",
    "        - Training Process\n",
    "   - 5.2. Hyperparameter Tuning\n",
    "        - Configurations Tested\n",
    "        - Tuning Methodology\n",
    "        - Rationale for Each Change\n",
    "   - 5.3. Results per Configuration\n",
    "        - Configuration 1: [name]\n",
    "        - Configuration 2: [name]\n",
    "        - Configuration 3: [name]\n",
    "   - 5.4. Best Model Selection\n",
    "\n",
    "### 6. Evaluation and Comparison (Week 3)\n",
    "   - 6.1. Evaluation Methodology\n",
    "        - Metrics Definition\n",
    "        - Evaluation Protocol\n",
    "   - 6.2. Quantitative Results\n",
    "        - Performance Comparison Table\n",
    "        - Statistical Significance Tests\n",
    "   - 6.3. Qualitative Analysis\n",
    "        - Behavioral Differences\n",
    "        - Play Style Comparison\n",
    "   - 6.4. Learning Curves Analysis\n",
    "        - Training Progression\n",
    "        - Convergence Comparison\n",
    "   - 6.5. Visualizations\n",
    "        - Reward Evolution\n",
    "        - Action Distributions\n",
    "        - Game Statistics\n",
    "\n",
    "### 7. Discussion\n",
    "   - 7.1. What Worked\n",
    "        - Successful Modifications\n",
    "        - Positive Impacts\n",
    "   - 7.2. What Didn't Work\n",
    "        - Failed Approaches\n",
    "        - Unexpected Behaviors\n",
    "   - 7.3. Insights and Learnings\n",
    "        - Reward Shaping Implications\n",
    "        - Trade-offs Discovered\n",
    "   - 7.4. Limitations\n",
    "        - Technical Constraints\n",
    "        - Time/Resource Limitations\n",
    "\n",
    "### 8. Conclusions\n",
    "   - 8.1. Summary of Findings\n",
    "   - 8.2. Impact of Customizations\n",
    "   - 8.3. Future Work\n",
    "\n",
    "### 9. References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39805a",
   "metadata": {},
   "source": [
    "***\n",
    "## **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784629a",
   "metadata": {},
   "source": [
    "### 1.1 Objective\n",
    "[inserir objetivo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bc0a5",
   "metadata": {},
   "source": [
    "### 1.2 Environment Selection: IceHockey-v5\n",
    "[Explicar o porquê de escolhermos IceHockey]\n",
    "- Two-player ice hockey game\n",
    "- Discrete action space (18 actions)\n",
    "- Sparse reward structure\n",
    "- Strategic gameplay requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4320d53",
   "metadata": {},
   "source": [
    "### 1.3 Motivation and Expected Outcomes\n",
    "[O que esperamos alcançar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbd9f5",
   "metadata": {},
   "source": [
    "***\n",
    "## **2. Environment Analysis**\n",
    "[Informação do ambiente](https://ale.farama.org/environments/ice_hockey/)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3f0e1",
   "metadata": {},
   "source": [
    "Neste ambiente controlamos uma equipa de 2 jogadores, para simular como era jogado o IceHockey nós só controlamos um dos jogadores de cada vez.<br>\n",
    "O jogador da equipa a ser controlado muda automaticamente conforme a proximidade do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c43758",
   "metadata": {},
   "source": [
    "### 2.1 Original Environment Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a648e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Information ===\n",
      "Observation Space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Action Space: Discrete(18)\n",
      "Action Meanings: ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"ALE/IceHockey-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"=== Environment Information ===\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Action Meanings: {env.unwrapped.get_action_meanings()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb2c64",
   "metadata": {},
   "source": [
    "A recompensa inicial deste ambiente é +1 por ponto marcado, -1 por ponto sofrido.<br>\n",
    "Um score negativo indica vitória do adversário e um positivo indica vitória da nossa equipa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b728f9c",
   "metadata": {},
   "source": [
    "### 2.2 Baseline Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a660711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Agent Performance:\n",
      "  Mean Reward: -9.46 ± 3.75\n",
      "  Mean Length: 3305.5\n"
     ]
    }
   ],
   "source": [
    "def test_random_agent(env, n_episodes=50):\n",
    "    \"\"\"Test random policy - basic stats\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    print(f\"Random Agent Performance:\")\n",
    "    print(f\"  Mean Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"  Mean Length: {np.mean(episode_lengths):.1f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "env = gym.make('ALE/IceHockey-v5')\n",
    "random_rewards, random_lengths = test_random_agent(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e415c15",
   "metadata": {},
   "source": [
    "### 2.3 Identified Modification Opportunities\n",
    "\n",
    "The current reward function used in the Ice Hockey environment is a purely sparse objective reward, providing:\n",
    "\n",
    "- +1 for scoring a goal\n",
    "\n",
    "- –1 for conceding a goal\n",
    "\n",
    "While this formulation captures the final objective of the game, it presents several limitations that directly hinder the learning performance of reinforcement learning agents, particularly in multi-agent and continuous-time domains. Based on insights from recent studies in MARL applied to robot football and simulated sports environments, several modification opportunities can be identified.\n",
    "\n",
    "\n",
    "1. **Sparse Rewards Are Ineffective in Long-Horizon, Multi-Agent Tasks**\n",
    "\n",
    "Sparse scoring events provide too little information for the agent to learn meaningful behaviours.  \n",
    "Multiple studies highlight the limitations of using goals as the only training signal.\n",
    "\n",
    "- *“Goals are infrequent events that provide little learning signal for credit assignment.”*  \n",
    "  — *Embedding Contextual Information Through Reward Shaping* :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "- *“The reward associated with scoring has low impact due to its rarity.”*  \n",
    "  — *The Role of a Reward in Shaping Multiple Football Agents’ Behavior* :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "***Opportunity:*** \n",
    "Introduce dense shaping rewards to provide feedback at every timestep, improving sample efficiency and stabilizing learning.\n",
    "\n",
    "\n",
    "2. **Lack of Intermediate Feedback Prevents Effective Credit Assignment**\n",
    "\n",
    "With only ±1 at terminal states, agents cannot distinguish which specific actions contributed to success or failure.\n",
    "\n",
    "- rSoccer results demonstrate that sparse rewards cause agents to *“fail to learn coordinated behaviours”* and that rewards become *“inadequate when observation dimensionality increases”* :contentReference[oaicite:2]{index=2}.\n",
    "\n",
    "- The GRF benchmark shows that adding checkpoint rewards dramatically improves learning stability:  \n",
    "  *“Dense reward shows better and more stable performance than sparse reward.”* :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "***Opportunity:*** \n",
    "Add intermediate tactical rewards (e.g., puck recovery, successful passes, shots on target).\n",
    "\n",
    "\n",
    "3. **No Directional Guidance Leads to Unstructured or Random Policies**\n",
    "\n",
    "The current reward provides no gradient for progress toward the opponent’s goal, leading to exploratory behaviours that do not contribute to scoring.\n",
    "\n",
    "In GRF, directional shaping (CHECKPOINT reward) was introduced for exactly this reason:\n",
    "\n",
    "- *“CHECKPOINTS reward substantially reduces exploratory actions by guiding the player toward the opponent’s goal.”*  \n",
    "  — GRF MARL Benchmark :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "***Opportunity:***  \n",
    "Include a **progress reward** based on puck displacement toward the opponent’s goal.\n",
    "\n",
    "\n",
    "4. **Lack of Coordination Incentives Produces Degenerate Multi-Agent Behavior**\n",
    "\n",
    "Sparse rewards do not encourage:\n",
    "\n",
    "- spacing  \n",
    "- defensive support  \n",
    "- coordinated pressure  \n",
    "- role differentiation  \n",
    "\n",
    "This problem is documented in rSoccer, where:\n",
    "\n",
    "- *“Agents block each other and fail to collaborate when no shaping is provided.”*  \n",
    "  — rSoccer Framework :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "***Opportunity:***  \n",
    "Add shaping terms encouraging cooperative behaviour such as spacing, puck recovery, and avoidance of collisions.\n",
    "\n",
    "\n",
    "5. **Sparse Rewards Lead to Overfitting and Non-Generalizable Strategies**\n",
    "\n",
    "Agents trained solely on goal rewards may learn brittle strategies that exploit specific opponent weaknesses rather than robust tactical principles.\n",
    "\n",
    "The GRF benchmark warns that:\n",
    "\n",
    "- *“Training solely against fixed opponents leads to overfitting; policies remain far from robust.”*  \n",
    "  — GRF Full-Game Analysis :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "***Opportunity:***  \n",
    "Reward behaviours that generalize across opponents (e.g., maintaining possession, structured defensive pressure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6eac01",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Environment Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182ac9e",
   "metadata": {},
   "source": [
    "### 3.1 Reward Engineering Design\n",
    "\n",
    "**Proposed Modifications:**<br>\n",
    "reward inicial = 0\n",
    "\n",
    "1. ***Golos:***\n",
    "if scored_goal: reward += 1.0\n",
    "if conceded_goal: reward -= 1.0\n",
    "\n",
    "2. ***Direção (inspirado CHECKPOINT):***\n",
    "reward += k1 * (old_dist_to_goal - new_dist_to_goal)\n",
    "\n",
    "3. ***Posse do puck:***\n",
    "if has_puck: reward += 0.01\n",
    "else: reward -= 0.01\n",
    "\n",
    "4. ***Pressão e defesa:***\n",
    "reward += k2 * (old_dist_to_puck - new_dist_to_puck)\n",
    "\n",
    "5. ***Ações úteis:***\n",
    "if completed_pass: reward += 0.1\n",
    "if shot_on_goal: reward += 0.05\n",
    "if steal_puck: reward += 0.2\n",
    "\n",
    "6. ***Penalizações:***\n",
    "if collision: reward -= 0.02\n",
    "if agent_idle_too_long: reward -= 0.02\n",
    "if teammates_too_close: reward -= 0.02\n",
    "\n",
    "[Explicar teoria por trás]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772c518",
   "metadata": {},
   "source": [
    "### 3.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b125914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir código :P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44c780",
   "metadata": {},
   "source": [
    "### 3.3 Testing Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f9f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver se funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a8e05",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Baseline Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb3bb8",
   "metadata": {},
   "source": [
    "### 4.1 Algorithm Selection\n",
    "É o mesmo que toda a gente <br>\n",
    "**Why DQN?**\n",
    "\n",
    "We chose Deep Q-Network (DQN) for the following reasons:\n",
    "\n",
    "1. **Image-based observations**: DQN with CNN policy is standard for Atari\n",
    "2. **Discrete actions**: DQN is designed for discrete action spaces\n",
    "3. **Proven track record**: DQN has strong performance on Atari games\n",
    "4. **Stable-Baselines3 support**: Well-implemented and documented\n",
    "\n",
    "**Alternative Considerations**\n",
    "- PPO: More stable but potentially slower convergence\n",
    "- A2C: Faster training but less sample efficient\n",
    "- Rainbow DQN: Better performance but more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fbb92",
   "metadata": {},
   "source": [
    "### 4.2 Baseline Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637aa9a4",
   "metadata": {},
   "source": [
    "### 4.3 Baseline Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16586516",
   "metadata": {},
   "source": [
    "### 4.4 Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dee330",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Custom Environment Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7bf0a",
   "metadata": {},
   "source": [
    "### 5.1 Training with custom rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb796b",
   "metadata": {},
   "source": [
    "### 5.2 (???) Hyperparameter tuning\n",
    "Vamos ter várias configurações focadas em coisas diferentes ex: learning rate, buffer, exploração"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc709a9",
   "metadata": {},
   "source": [
    "### 5.3 Results per configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87523d",
   "metadata": {},
   "source": [
    "### 5.4 Best model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13269f0",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca801b1",
   "metadata": {},
   "source": [
    "### 6.1 Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439faa7e",
   "metadata": {},
   "source": [
    "### 6.2 Quant results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9c70a",
   "metadata": {},
   "source": [
    "### 6.3 Qual results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d26c8c",
   "metadata": {},
   "source": [
    "### 6.4 Learning curve analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23062c",
   "metadata": {},
   "source": [
    "### 6.5 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f890e",
   "metadata": {},
   "source": [
    "***\n",
    "## 7. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1734fc",
   "metadata": {},
   "source": [
    "***\n",
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ad627",
   "metadata": {},
   "source": [
    "***\n",
    "## 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b812f01",
   "metadata": {},
   "source": [
    "[1] Song, Y. et al. (2023). The Role of a Reward in Shaping Multiple Football Agents’ Behavior. <br>\n",
    "[2] Zhang, H. et al. (2023). Embedding Contextual Information through Reward Shaping in Multi-Agent Learning. <br>\n",
    "[3] Martins, F. B. et al. (2021). rSoccer: A Framework for Studying Reinforcement Learning. <br>\n",
    "[4] Song, Y. et al. (2024). Boosting Studies of Multi-Agent RL on Google Research Football."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
